{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danielp8s/DLP2/blob/main/Op2_DL2_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yNL-xhHWrp8"
      },
      "outputs": [],
      "source": [
        "#PARTE 1\n",
        "# Importar las bibliotecas necesarias\n",
        "import os                                             # Operaciones del sistema (rutas de archivos, directorios)\n",
        "import torch                                          # Pytorch: Biblioteca principal para crear y entrenar redes neuronales\n",
        "import torchvision                                    # Paquete para trabajar con imágenes y modelos preentrenados\n",
        "from torchvision import datasets, transforms, models  # Para gestionar datasets y realizar transformaciones en imágenes\n",
        "from torch.utils.data import DataLoader, Subset       # Para gestionar lotes de datos (dataloaders) y subconjuntos\n",
        "import torch.nn as nn                                 # Para definir la estructura de la red neuronal (capas)\n",
        "import torch.optim as optim                           # Para implementar algoritmos de optimización (Adam, SGD)\n",
        "import matplotlib.pyplot as plt                       # Para graficar los resultados\n",
        "from sklearn.model_selection import train_test_split  # Para dividir el conjunto de datos en entrenamiento y validación\n",
        "from sklearn.metrics import classification_report, confusion_matrix  # Para medir el rendimiento del modelo\n",
        "import numpy as np                                    # Para cálculos y manipulación de arrays\n",
        "\n",
        "# Verificar disponibilidad de GPU\n",
        "# Si se dispone de una GPU compatible con CUDA, se utilizará; de lo contrario, se usará la CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTE 2\n",
        "# Montar Google Drive para acceder a los datos (solo en Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os  # Para manejar rutas de archivos y directorios\n",
        "\n",
        "# Definir la ruta donde están los datos (carpeta \"dogs_vs_cats\" en Google Drive)\n",
        "data_dir = '/content/drive/MyDrive/dogs_vs_cats'\n",
        "\n",
        "# TRANSFORMACIONES PARA LOS DATOS\n",
        "# Transformación de entrenamiento para VGG16 y ResNet50 (tamaño de entrada 128x128)\n",
        "train_transforms_small = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(128),               # Recorte aleatorio y redimensionado a 128x128\n",
        "    transforms.RandomHorizontalFlip(),               # Volteo horizontal aleatorio\n",
        "    transforms.ToTensor(),                           # Convertir la imagen a tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],      # Normalización con valores medios y desviaciones estándar (ImageNet)\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Transformación de prueba para VGG16 y ResNet50 (128x128)\n",
        "test_transforms_small = transforms.Compose([\n",
        "    transforms.Resize(150),                          # Redimensionar la imagen a 150px\n",
        "    transforms.CenterCrop(128),                      # Recorte centrado a 128x128\n",
        "    transforms.ToTensor(),                           # Convertir a tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],      # Normalización (valores de ImageNet)\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Transformación de entrenamiento para Inceptionv3 (tamaño de entrada 299x299)\n",
        "train_transforms_large = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(299),               # Recorte aleatorio y redimensionado a 299x299\n",
        "    transforms.RandomHorizontalFlip(),               # Volteo horizontal aleatorio\n",
        "    transforms.ToTensor(),                           # Convertir a tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],      # Normalización (valores de ImageNet)\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Transformación de prueba para Inceptionv3 (299x299)\n",
        "test_transforms_large = transforms.Compose([\n",
        "    transforms.Resize(299),                          # Redimensionar la imagen a 299px\n",
        "    transforms.CenterCrop(299),                      # Recorte centrado a 299x299\n",
        "    transforms.ToTensor(),                           # Convertir a tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],      # Normalización (valores de ImageNet)\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Cargar las imágenes de entrenamiento desde la carpeta \"train\"\n",
        "train_dir = os.path.join(data_dir, 'train')  # Ruta de las imágenes de entrenamiento\n",
        "test_dir = os.path.join(data_dir, 'test')    # Ruta de las imágenes de prueba\n",
        "\n",
        "# Cargar el dataset con las transformaciones correspondientes (para VGG16 y ResNet50)\n",
        "train_dataset_small = datasets.ImageFolder(train_dir, transform=train_transforms_small)\n",
        "\n",
        "# Cargar el dataset para Inceptionv3 (transformaciones con 299x299)\n",
        "train_dataset_large = datasets.ImageFolder(train_dir, transform=train_transforms_large)\n",
        "\n",
        "# Dividir el dataset de entrenamiento en subconjuntos de entrenamiento (75%) y validación (25%)\n",
        "train_size = int(0.75 * len(train_dataset_small))  # 75% de los datos para entrenamiento\n",
        "val_size = len(train_dataset_small) - train_size   # 25% para validación\n",
        "\n",
        "# Generar índices aleatorios para dividir el conjunto de datos\n",
        "train_indices, val_indices = train_test_split(range(len(train_dataset_small)), test_size=val_size, random_state=42)\n",
        "\n",
        "# Crear subconjuntos de entrenamiento y validación (para VGG16 y ResNet50)\n",
        "train_subset_small = Subset(train_dataset_small, train_indices)  # Subconjunto de entrenamiento\n",
        "val_subset_small = Subset(train_dataset_small, val_indices)      # Subconjunto de validación\n",
        "\n",
        "# Crear subconjuntos de entrenamiento y validación (para Inceptionv3)\n",
        "train_subset_large = Subset(train_dataset_large, train_indices)  # Subconjunto de entrenamiento\n",
        "val_subset_large = Subset(train_dataset_large, val_indices)      # Subconjunto de validación\n",
        "\n",
        "# Crear DataLoaders para cargar los datos en lotes pequeños (batch size 8)\n",
        "batch_size = 8  # Tamaño del lote\n",
        "\n",
        "# DataLoaders para VGG16 y ResNet50\n",
        "train_loader_small = DataLoader(train_subset_small, batch_size=batch_size, shuffle=True)  # Conjunto de entrenamiento\n",
        "val_loader_small = DataLoader(val_subset_small, batch_size=batch_size, shuffle=False)     # Conjunto de validación\n",
        "\n",
        "# DataLoaders para Inceptionv3\n",
        "train_loader_large = DataLoader(train_subset_large, batch_size=batch_size, shuffle=True)  # Conjunto de entrenamiento\n",
        "val_loader_large = DataLoader(val_subset_large, batch_size=batch_size, shuffle=False)     # Conjunto de validación\n",
        "\n",
        "# Cargar los datasets de prueba para VGG16/ResNet50 y Inceptionv3\n",
        "test_dataset_small = datasets.ImageFolder(test_dir, transform=test_transforms_small)      # Dataset de prueba (VGG16/ResNet50)\n",
        "test_dataset_large = datasets.ImageFolder(test_dir, transform=test_transforms_large)      # Dataset de prueba (Inceptionv3)\n",
        "\n",
        "# DataLoaders para los conjuntos de prueba\n",
        "test_loader_small = DataLoader(test_dataset_small, batch_size=batch_size, shuffle=False)  # Lotes de datos de prueba (VGG16/ResNet50)\n",
        "test_loader_large = DataLoader(test_dataset_large, batch_size=batch_size, shuffle=False)  # Lotes de datos de prueba (Inceptionv3)\n"
      ],
      "metadata": {
        "id": "N5IeI1sTW_E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTE 3\n",
        "# DEFINIR LOS MODELOS PREENTRENADOS\n",
        "\n",
        "# Cargar modelos preentrenados de VGG16, ResNet50 e InceptionV3\n",
        "vgg16 = models.vgg16(pretrained=True)  # Carga el modelo VGG16 con pesos preentrenados\n",
        "resnet50 = models.resnet50(pretrained=True)  # Carga el modelo ResNet50 con pesos preentrenados\n",
        "inceptionv3 = models.inception_v3(pretrained=True)  # Carga el modelo InceptionV3 con pesos preentrenados\n",
        "\n",
        "# Congelar capas iniciales para que no se actualicen durante el entrenamiento\n",
        "for param in vgg16.features.parameters():\n",
        "    param.requires_grad = False           # No actualizar parámetros de VGG16\n",
        "for param in resnet50.parameters():\n",
        "    param.requires_grad = False           # No actualizar parámetros de ResNet50\n",
        "for param in inceptionv3.parameters():\n",
        "    param.requires_grad = False           # No actualizar parámetros de InceptionV3\n",
        "\n",
        "# FINE TUNNING - Ajuste fino: cambiar la capa de salida para clasificación binaria\n",
        "vgg16.classifier[6] = nn.Linear(4096, 2)  # Cambiar la capa de salida de VGG16 para 2 clases (perro/gato)\n",
        "resnet50.fc = nn.Linear(resnet50.fc.in_features, 2)  # Cambiar la capa de salida de ResNet50 para 2 clases\n",
        "inceptionv3.fc = nn.Linear(inceptionv3.fc.in_features, 2)  # Cambiar la capa de salida de InceptionV3 para 2 clases\n",
        "\n",
        "# Mover los modelos a la GPU para aceleración en el entrenamiento\n",
        "vgg16 = vgg16.to(device)                  # Mover VGG16 a GPU\n",
        "resnet50 = resnet50.to(device)            # Mover ResNet50 a GPU\n",
        "inceptionv3 = inceptionv3.to(device)      # Mover InceptionV3 a GPU\n"
      ],
      "metadata": {
        "id": "L1yj9VKNXP5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTE 4\n",
        "# SELECCIÓN DE HIPERPARÁMETROS Y CONFIGURACIÓN DE ENTRENAMIENTO\n",
        "\n",
        "# Variación de Hiperparámetros\n",
        "learning_rate = 0.001                 # Tasa de aprendizaje para el optimizador\n",
        "num_epochs = 2                        # Número de épocas para el entrenamiento\n",
        "\n",
        "# Definir los optimizadores y la función de pérdida\n",
        "criterion = nn.CrossEntropyLoss()     # Función de pérdida para clasificación múltiple (Cross Entropy Loss)\n",
        "\n",
        "# Crear optimizadores Adam para cada modelo con la tasa de aprendizaje definida\n",
        "optimizer_vgg16 = optim.Adam(vgg16.parameters(), lr=learning_rate)              # Optimizador para VGG16\n",
        "optimizer_resnet50 = optim.Adam(resnet50.parameters(), lr=learning_rate)        # Optimizador para ResNet50\n",
        "optimizer_inceptionv3 = optim.Adam(inceptionv3.parameters(), lr=learning_rate)  # Optimizador para InceptionV3\n"
      ],
      "metadata": {
        "id": "UzZKbY4_Xf9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTE 5\n",
        "# FUNCIONES DE ENTRENAMIENTO Y EVALUACIÓN\n",
        "\n",
        "# Función de entrenamiento\n",
        "def train(model, dataloader, criterion, optimizer, is_inception=False):\n",
        "    model.train()           # Cambiar a modo de entrenamiento\n",
        "    running_loss = 0.0      # Inicializar pérdida acumulada\n",
        "    running_corrects = 0    # Inicializar conteo de aciertos\n",
        "\n",
        "    # Iterar sobre el DataLoader\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Mover los datos a la GPU\n",
        "\n",
        "        optimizer.zero_grad()  # Limpiar los gradientes acumulados\n",
        "\n",
        "        # Forward pass\n",
        "        if is_inception:  # Si se está utilizando el modelo InceptionV3\n",
        "\n",
        "            # En InceptionV3, se obtienen dos salidas (la principal y la auxiliar)\n",
        "            outputs, aux_outputs = model(inputs)      # Obtener las predicciones\n",
        "            loss1 = criterion(outputs, labels)        # Calcular la pérdida principal\n",
        "            loss2 = criterion(aux_outputs, labels)    # Calcular la pérdida auxiliar\n",
        "            loss = loss1 + 0.4 * loss2                # Dar menor peso a la pérdida auxiliar\n",
        "        else:\n",
        "            outputs = model(inputs)            # Obtener las predicciones\n",
        "            loss = criterion(outputs, labels)  # Calcular la pérdida\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)  # Obtener las predicciones con mayor probabilidad\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()   # Calcular los gradientes\n",
        "        optimizer.step()  # Actualizar los parámetros del modelo\n",
        "\n",
        "        # Calcular pérdida y precisión acumuladas\n",
        "        running_loss += loss.item() * inputs.size(0)  # Acumular pérdida\n",
        "        running_corrects += torch.sum(preds == labels.data)  # Acumular aciertos\n",
        "\n",
        "    # Calcular pérdida y precisión por época\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)              # Pérdida media por época\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)  # Precisión media por época\n",
        "    return epoch_loss, epoch_acc                                      # Retornar pérdida y precisión\n",
        "\n",
        "# Función de evaluación\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()              # Cambiar a modo de evaluación\n",
        "    running_loss = 0.0        # Inicializar pérdida acumulada\n",
        "    correct = 0               # Inicializar conteo de aciertos\n",
        "    total = 0                 # Inicializar conteo total de imágenes\n",
        "\n",
        "    with torch.no_grad():  # Desactivar cálculo de gradientes\n",
        "        # Iterar sobre el DataLoader\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Mover los datos a la GPU\n",
        "            outputs = model(images)                                # Obtener las predicciones\n",
        "            loss = criterion(outputs, labels)                      # Calcular la pérdida\n",
        "\n",
        "            running_loss += loss.item()                            # Acumular pérdida\n",
        "            _, predicted = torch.max(outputs, 1)                   # Obtener las predicciones con mayor probabilidad\n",
        "            total += labels.size(0)                                # Actualizar el conteo total\n",
        "            correct += (predicted == labels).sum().item()          # Acumular aciertos\n",
        "\n",
        "    accuracy = 100 * correct / total             # Calcular la precisión en porcentaje\n",
        "    return running_loss / len(loader), accuracy  # Retornar pérdida y precisión\n"
      ],
      "metadata": {
        "id": "HdWYOcNKXu_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTE 6\n",
        "# ENTRENAMIENTO DE LOS MODELOS\n",
        "\n",
        "# Almacenar historial de precisión y pérdida\n",
        "history = {'vgg16': [], 'resnet50': [], 'inceptionv3': []}  # Diccionario para guardar resultados de cada modelo\n",
        "\n",
        "# Entrenar y evaluar los modelos\n",
        "for epoch in range(num_epochs):             # Iterar sobre el número de épocas\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")  # Imprimir número de época actual\n",
        "    print(\"-\" * 10)                         # Separador visual\n",
        "\n",
        "    # VGG16\n",
        "    train_loss, train_acc = train(vgg16, train_loader_small, criterion, optimizer_vgg16)  # Entrenar el modelo VGG16\n",
        "    val_loss, val_acc = evaluate(vgg16, val_loader_small, criterion)                      # Evaluar el modelo VGG16\n",
        "    history['vgg16'].append((train_loss, train_acc, val_loss, val_acc))                   # Almacenar resultados\n",
        "    print(f\"VGG16 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}\")  # Imprimir resultados\n",
        "\n",
        "    # ResNet50\n",
        "    train_loss, train_acc = train(resnet50, train_loader_small, criterion, optimizer_resnet50)  # Entrenar el modelo ResNet50\n",
        "    val_loss, val_acc = evaluate(resnet50, val_loader_small, criterion)                         # Evaluar el modelo ResNet50\n",
        "    history['resnet50'].append((train_loss, train_acc, val_loss, val_acc))                      # Almacenar resultados\n",
        "    print(f\"ResNet50 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}\")  # Imprimir resultados\n",
        "\n",
        "    # Inceptionv3\n",
        "    train_loss, train_acc = train(inceptionv3, train_loader_large, criterion, optimizer_inceptionv3, is_inception=True)  # Entrenar el modelo InceptionV3\n",
        "    val_loss, val_acc = evaluate(inceptionv3, val_loader_large, criterion)                      # Evaluar el modelo InceptionV3\n",
        "    history['inceptionv3'].append((train_loss, train_acc, val_loss, val_acc))                   # Almacenar resultados\n",
        "    print(f\"Inceptionv3 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}\")  # Imprimir resultados\n"
      ],
      "metadata": {
        "id": "JV2VXbGTYZkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTE 7\n",
        "# PREDICCIÓN EN EL CONJUNTO DE PRUEBA\n",
        "\n",
        "# Función para realizar predicciones\n",
        "def predict(model, dataloader):   # Definición de la función de predicción\n",
        "    model.eval()                  # Configurar el modelo en modo de evaluación\n",
        "    all_preds = []                # Lista para almacenar todas las predicciones\n",
        "    with torch.no_grad():         # Desactivar el cálculo del gradiente\n",
        "        for images, _ in dataloader:          # Iterar sobre los datos del dataloader\n",
        "            images = images.to(device)        # Mover imágenes a la GPU\n",
        "            outputs = model(images)           # Realizar la predicción\n",
        "            _, preds = torch.max(outputs, 1)  # Obtener las predicciones (índices de clase)\n",
        "            all_preds.extend(preds.cpu().numpy())  # Almacenar las predicciones en la lista\n",
        "    return all_preds  # Retornar todas las predicciones\n",
        "\n",
        "# Obtener predicciones\n",
        "preds_vgg16 = predict(vgg16, test_loader_small)              # Predicciones del modelo VGG16 en el conjunto de prueba\n",
        "preds_resnet50 = predict(resnet50, test_loader_small)        # Predicciones del modelo ResNet50 en el conjunto de prueba\n",
        "preds_inceptionv3 = predict(inceptionv3, test_loader_large)  # Predicciones del modelo InceptionV3 en el conjunto de prueba\n"
      ],
      "metadata": {
        "id": "JQD7EH89Ys3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTE 8\n",
        "# GENERAR INFORMES DE CLASIFICACIÓN\n",
        "\n",
        "# Función para imprimir el informe de clasificación\n",
        "def print_classification_report(y_true, y_pred):  # Definición de la función que toma las etiquetas verdaderas y predicciones\n",
        "    print(classification_report(y_true, y_pred, target_names=test_dataset_small.classes))  # Imprimir el informe de clasificación con nombres de clases\n",
        "\n",
        "# Cálculo de la precisión en el conjunto de prueba\n",
        "test_labels = test_dataset_small.targets               # Obtener etiquetas verdaderas del conjunto de prueba\n",
        "print(\"VGG16 Classification Report:\")                  # Imprimir encabezado para el informe de VGG16\n",
        "print_classification_report(test_labels, preds_vgg16)  # Generar y mostrar el informe para VGG16\n",
        "\n",
        "print(\"ResNet50 Classification Report:\")                  # Imprimir encabezado para el informe de ResNet50\n",
        "print_classification_report(test_labels, preds_resnet50)  # Generar y mostrar el informe para ResNet50\n",
        "\n",
        "print(\"InceptionV3 Classification Report:\")                  # Imprimir encabezado para el informe de InceptionV3\n",
        "print_classification_report(test_labels, preds_inceptionv3)  # Generar y mostrar el informe para InceptionV3\n"
      ],
      "metadata": {
        "id": "TUAZH1EaY6AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTE 9\n",
        "# VISUALIZACIÓN DE RESULTADOS DE PERDIDA Y PRECISIÓN\n",
        "\n",
        "# Función para graficar la historia de entrenamiento\n",
        "def plot_history(history):                       # Definición de la función que toma el historial de métricas\n",
        "    for model_name, metrics in history.items():  # Iterar sobre los modelos y sus métricas\n",
        "        train_losses, train_accs, val_losses, val_accs = zip(*metrics)  # Desempaquetar las métricas en listas separadas\n",
        "        plt.figure(figsize=(12, 5))              # Crear una nueva figura con tamaño específico\n",
        "\n",
        "        # Pérdida\n",
        "        plt.subplot(1, 2, 1)                           # Crear un subplot para la pérdida\n",
        "        plt.plot(train_losses, label='Training Loss')  # Graficar la pérdida de entrenamiento\n",
        "        plt.plot(val_losses, label='Validation Loss')  # Graficar la pérdida de validación\n",
        "        plt.title(f'{model_name} Loss')                # Título del gráfico de pérdida\n",
        "        plt.xlabel('Epoch')                            # Etiqueta del eje x\n",
        "        plt.ylabel('Loss')                             # Etiqueta del eje y\n",
        "        plt.legend()                                   # Mostrar la leyenda\n",
        "\n",
        "        # Precisión\n",
        "        plt.subplot(1, 2, 2)                             # Crear un subplot para la precisión\n",
        "        plt.plot(train_accs, label='Training Accuracy')  # Graficar la precisión de entrenamiento\n",
        "        plt.plot(val_accs, label='Validation Accuracy')  # Graficar la precisión de validación\n",
        "        plt.title(f'{model_name} Accuracy')              # Título del gráfico de precisión\n",
        "        plt.xlabel('Epoch')                              # Etiqueta del eje x\n",
        "        plt.ylabel('Accuracy')                           # Etiqueta del eje y\n",
        "        plt.legend()                                     # Mostrar la leyenda\n",
        "\n",
        "        plt.tight_layout()  # Ajustar el diseño para que no se superpongan\n",
        "        plt.show()          # Mostrar el gráfico\n",
        "\n",
        "plot_history(history)  # Llamar a la función para graficar el historial de entrenamiento\n"
      ],
      "metadata": {
        "id": "4xdjgsh2ZJff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}